{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Inicializando Ambiente Django no Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por enquanto não estou utilizando os dados do banco de dados (e sim o csv presente em csv_data/).\n",
    "\n",
    "Porém no dia 21/05/25 modifiquei o django_for_jupyter.py para que após a reestruturação dos diretórios os notebooks antigos (e também os dentro de classification) possam acessar o BD futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Django initialized for project: app\n",
      "Project root added to sys.path: /home/esdras-daniel/Documentos/Python/Django/PGM-Text_Classificator\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from django_for_jupyter import init_django\n",
    "init_django('app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminho do CSV: /home/esdras-daniel/Documentos/Python/Django/PGM-Text_Classificator/notebooks/../csv_data/processed/pgm-dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "csv_file_path = os.path.join(notebook_dir, '..', 'csv_data', 'processed', 'pgm-dataset_clean.csv')\n",
    "print(f'Caminho do CSV: {csv_file_path}')\n",
    "\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de stopwords em português: 207\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar a lista de stopwords (necessário apenas na primeira vez)\n",
    "try:\n",
    "    stopwords.words('portuguese')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stopwords_pt = stopwords.words('portuguese')\n",
    "print(f'Total de stopwords em português: {len(stopwords_pt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower() # Converter para minúsculas para padronização\n",
    "\n",
    "    # 1. Remover acentos (nomalização unicode)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # 2. Regex\n",
    "    text = re.sub(r'poder judiciario(?: do)? estado(:? do)? rio grande(:? do)? norte', '', text)\n",
    "    text = re.sub(r'\\d{7}-\\d{2}\\.\\d{4}\\.\\d{1,2}\\.\\d{2}\\.\\d{4}', '[Nº_PROCESSO]', text)\n",
    "    text = re.sub(r'\\(\\d{2}\\)\\s?\\d{4,5}-?\\d{4}', '[TELEFONE]', text)\n",
    "    text = re.sub(r'xnone', '', text)\n",
    "\n",
    "    # 3. Tokenização e remoção de stop words\n",
    "    words = text.split()\n",
    "    processed_words = [word for word in words if word not in stopwords_pt]\n",
    "\n",
    "    # 4. Remover pontuação e caracteres não alfa númericos\n",
    "    processed_words = [re.sub(r'[^a-z0-9]', '', word) for word in processed_words]\n",
    "    processed_words = [word for word in processed_words if word] # Remover strings vazias\n",
    "\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de textos para fazer o embedding: 14926\n"
     ]
    }
   ],
   "source": [
    "texts_to_embed = df['teorTexto'].apply(preprocess_text).to_numpy()\n",
    "print(f'Qtd de textos para fazer o embedding: {len(texts_to_embed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gerando Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando embeddings com o modelo \"rufimelo/Legal-BERTimbau-base\" usando agregação \"mean\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rufimelo/Legal-BERTimbau-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 14926/14926 [2:46:17<00:00,  1.50it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings salvos em 'embeddings/rufimelo-Legal-BERTimbau-base_mean.pt' com shape torch.Size([14926, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# 1. Parâmetros configuráveis\n",
    "BERT_MODEL = 'rufimelo/Legal-BERTimbau-base'\n",
    "AGG_STRATEGY = 'mean'\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "# 2. Caminho de saída\n",
    "embeddings_dir = 'embeddings'\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "embedding_path = os.path.join(embeddings_dir, f'{BERT_MODEL.replace('/', '-')}_{AGG_STRATEGY}.pt')\n",
    "\n",
    "# 3. Verificação de existência\n",
    "if os.path.exists(embedding_path):\n",
    "    print(f'Embedding já existe em \"{embedding_path}\". Nenhuma ação necessária.')\n",
    "else:\n",
    "    # 4. Carregar modelo e tokenizer\n",
    "    print(f'Gerando embeddings com o modelo \"{BERT_MODEL}\" usando agregação \"{AGG_STRATEGY}\"...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "    model = AutoModel.from_pretrained(BERT_MODEL)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Função para dividir texto em chunks\n",
    "    def split_in_chunks(input_ids, max_tokens=MAX_TOKENS):\n",
    "\n",
    "        # Reservar espaço para [CLS] e [SEP]\n",
    "        chunk_size = max_tokens - 2\n",
    "        chunks = [input_ids[i:i+chunk_size] for i in range(0, len(input_ids), chunk_size)]\n",
    "        return chunks\n",
    "    \n",
    "    embeddings_finais = []\n",
    "\n",
    "    # Obter dimensão dos embeddings (por segurança após carregar modelo)\n",
    "    with torch.no_grad():\n",
    "        dummy_input = tokenizer(\"texto de teste\", return_tensors=\"pt\", max_length=MAX_TOKENS, truncation=True)\n",
    "        dummy_output = model(**dummy_input.to(device))\n",
    "        embedding_size = dummy_output.last_hidden_state.shape[-1]\n",
    "\n",
    "    # 6. Embeddings por texto\n",
    "    for text in tqdm(texts_to_embed):\n",
    "        if not text.strip():\n",
    "            zero_emb = torch.zeros(embedding_size)\n",
    "            embeddings_finais.append(zero_emb)\n",
    "            continue\n",
    "\n",
    "        tokens_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "        if len(tokens_ids) + 2 > MAX_TOKENS:\n",
    "            chunks = split_in_chunks(tokens_ids, MAX_TOKENS)\n",
    "        else:\n",
    "            chunks = [tokens_ids]\n",
    "\n",
    "        chunk_embedding = []\n",
    "\n",
    "        for chunk_ids in chunks:\n",
    "            # Adiciona os tokens [CLS] e [SEP]\n",
    "            chunks_ids = [tokenizer.cls_token_ids] + chunk_ids + [tokenizer.sep_token_ids]\n",
    "            attention_mask = [1] * len(chunk_ids)\n",
    "\n",
    "            # Converte para tensores\n",
    "            input_dict = {\n",
    "                'input_ids': torch.tensor([chunk_ids], device=device),\n",
    "                'attention_mask': torch.tensor([attention_mask], device=device)\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(**input_dict)\n",
    "                cls_embedding = output.last_hidden_state[:, 0, :]\n",
    "                chunk_embedding.append(cls_embedding.squeeze(0))\n",
    "        \n",
    "        if chunk_embedding:\n",
    "            chunks_embeddings_tensor = torch.stack(chunk_embedding)\n",
    "            if AGG_STRATEGY == 'mean':\n",
    "                emb = torch.mean(chunks_embeddings_tensor, dim=0)\n",
    "            elif AGG_STRATEGY == 'max':\n",
    "                emb = torch.max(chunks_embeddings_tensor, dim=0).values\n",
    "            else:\n",
    "                raise ValueError(\"Estratégia inválida. Use 'mean' ou 'max'.\")\n",
    "            embeddings_finais.append(emb.cpu())\n",
    "        else:\n",
    "            embeddings_finais.append(torch.zeros(embedding_size))\n",
    "\n",
    "    # 7. Salvar\n",
    "    todos_embeddings = torch.stack(embeddings_finais)\n",
    "    torch.save(todos_embeddings, embedding_path)\n",
    "    print(f\"Embeddings salvos em '{embedding_path}' com shape {todos_embeddings.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
